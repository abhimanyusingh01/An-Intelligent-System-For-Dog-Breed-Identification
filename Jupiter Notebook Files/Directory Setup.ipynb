{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from utils import *\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from scipy import ndimage, io, misc\n",
    "from xml.dom import minidom\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Garrick\\\\Documents\\\\Springboard\\\\Capstone Project 2\\\\datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_create(path):\n",
    "    if  not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_create('train')\n",
    "file_create('test')\n",
    "file_create('cropped')\n",
    "file_create('cropped/train')\n",
    "file_create('cropped/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://vision.stanford.edu/aditya86/ImageNetDogs/'\n",
    "last_percent_reported = None\n",
    "data_root = '.'\n",
    "num_classes = 120\n",
    "image_size = 224\n",
    "num_channels = 3\n",
    "np.random.seed(133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    \"\"\"\n",
    "    A hook to report the progress of a download. This is mostly intended for users with\n",
    "    slow internet connections. Reports every 5% change in download progress.\n",
    "    \"\"\"\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    last_percent_reported = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    \"\"\"\n",
    "    Download a file if not present, and make sure it's the right size.\n",
    "    \"\"\"\n",
    "    dest_filename = os.path.join(data_root, filename)\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print('Attempting to download:', filename) \n",
    "        filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload Complete!')\n",
    "    statinfo = os.stat(dest_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', dest_filename)\n",
    "    else:\n",
    "        raise Exception('Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "    return dest_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_extract(filename, check_classes=True, force=False):\n",
    "    root = os.path.splitext(filename)[0]  # remove .tar\n",
    "    if os.path.isdir(root) and not force:\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(data_root)\n",
    "        tar.close()\n",
    "    if check_classes:\n",
    "        data_folders = [os.path.join(root, d) for d in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, d))]\n",
    "        if len(data_folders) != num_classes:\n",
    "            raise Exception('Expected %d folders, one per class. Found %d instead.' % (num_classes, len(data_folders)))\n",
    "        print('Completed extraction of %s.' % filename)\n",
    "        return data_folders\n",
    "    else:\n",
    "        print('Completed extraction of %s.' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified .\\images.tar\n",
      "Found and verified .\\annotation.tar\n",
      "Found and verified .\\lists.tar\n"
     ]
    }
   ],
   "source": [
    "images_filename = maybe_download('images.tar', 793579520)\n",
    "annotation_filename = maybe_download('annotation.tar', 21852160)\n",
    "lists_filename = maybe_download('lists.tar', 481280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_filename = 'images.tar'\n",
    "annotation_filename = 'annotation.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for images. This may take a while. Please wait.\n",
      "Completed extraction of images.tar.\n",
      "annotation already present - Skipping extraction of annotation.tar.\n",
      "Completed extraction of annotation.tar.\n",
      "Extracting data for .\\lists. This may take a while. Please wait.\n",
      "Completed extraction of .\\lists.tar.\n"
     ]
    }
   ],
   "source": [
    "images_folders = maybe_extract(images_filename)\n",
    "annotation_folders = maybe_extract(annotation_filename)\n",
    "maybe_extract(lists_filename, check_classes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for folder in images_folders:\n",
    "    os.makedirs(\"train/\"+folder.split(\"\\\\\")[-1])\n",
    "    os.makedirs(\"test/\"+folder.split(\"\\\\\")[-1])\n",
    "    os.makedirs(\"cropped/train/\"+folder.split(\"\\\\\")[-1])\n",
    "    os.makedirs(\"cropped/test/\"+folder.split(\"\\\\\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = io.loadmat('test_list.mat')['file_list']\n",
    "train_list = io.loadmat('train_list.mat')['file_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move_data_files(image_list, new_folder):\n",
    "    for file in image_list:\n",
    "        if os.path.exists('Images/'+file[0][0]):\n",
    "            shutil.move('Images/'+file[0][0],new_folder+'/'+file[0][0])\n",
    "        elif not os.path.exists(new_folder+'/'+file[0][0]):\n",
    "           print('%s does not exist, it may be missing' % os.path.exists('./images/'+file[0][0]))\n",
    "    return [new_folder+'/'+d for d in sorted(os.listdir(new_folder)) if os.path.isdir(os.path.join(new_folder, d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_folders = move_data_files(test_list, 'test')\n",
    "train_folders = move_data_files(train_list, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_breed(folder):\n",
    "    \"\"\"\n",
    "    Load the data for a single breed label.\n",
    "    \"\"\"\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size,num_channels), dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = folder+'/'+image\n",
    "        try:\n",
    "            \n",
    "            image_data = misc.imread(image_file)\n",
    "            \n",
    "            annon_file = 'Annotation' + '/' + folder.split('/')[-1] + '/' + image.split('.')[0]\n",
    "            annon_xml = minidom.parse(annon_file)\n",
    "            xmin = int(annon_xml.getElementsByTagName('xmin')[0].firstChild.nodeValue)\n",
    "            ymin = int(annon_xml.getElementsByTagName('ymin')[0].firstChild.nodeValue)\n",
    "            xmax = int(annon_xml.getElementsByTagName('xmax')[0].firstChild.nodeValue)\n",
    "            ymax = int(annon_xml.getElementsByTagName('ymax')[0].firstChild.nodeValue)\n",
    "            \n",
    "            new_image_data = image_data[ymin:ymax,xmin:xmax,:]\n",
    "            new_image_data = misc.imresize(new_image_data, (image_size, image_size))\n",
    "            misc.imsave('cropped/' + folder + '/' + image, new_image_data)\n",
    "            dataset[num_images, :, :, :] = new_image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "    dataset = dataset[0:num_images, :, :, :]\n",
    "\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cropped/test/n02085620-Chihuahua/n02085620_588.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-945ab24709f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cropped/test/n02085620-Chihuahua/n02085620_588.jpg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m229\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m229\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\garrick\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, target_size, interpolation)\u001b[0m\n\u001b[0;32m    360\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    361\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[1;32m--> 362\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'L'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\garrick\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2542\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2543\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2544\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cropped/test/n02085620-Chihuahua/n02085620_588.jpg'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "i=image.load_img('cropped/test/n02085620-Chihuahua/n02085620_588.jpg',target_size=(229,229))\n",
    "image.img_to_array(i).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_pickle(data_folders, force=False):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_breed(folder)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "    return dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/n02085620-Chihuahua\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\garrick\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  del sys.path[0]\n",
      "c:\\users\\garrick\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "c:\\users\\garrick\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:24: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset tensor: (100, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_breed('train/n02085620-Chihuahua')\n",
    "with open('n02085620-Chihuahua.pickle', 'wb') as f:\n",
    "    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_folders=os.listdir('train')\n",
    "train_folders=['train'+'/'+d for d in train_folders]\n",
    "test_folders=os.listdir('test')\n",
    "test_folders=['test'+'/'+d for d in test_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling train/n02085620-Chihuahua.pickle.\n",
      "train/n02085620-Chihuahua\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\garrick\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  del sys.path[0]\n",
      "c:\\users\\garrick\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "c:\\users\\garrick\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:24: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02085782-Japanese_spaniel.pickle.\n",
      "train/n02085782-Japanese_spaniel\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02085936-Maltese_dog.pickle.\n",
      "train/n02085936-Maltese_dog\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02086079-Pekinese.pickle.\n",
      "train/n02086079-Pekinese\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02086240-Shih-Tzu.pickle.\n",
      "train/n02086240-Shih-Tzu\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02086646-Blenheim_spaniel.pickle.\n",
      "train/n02086646-Blenheim_spaniel\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02086910-papillon.pickle.\n",
      "train/n02086910-papillon\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02087046-toy_terrier.pickle.\n",
      "train/n02087046-toy_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02087394-Rhodesian_ridgeback.pickle.\n",
      "train/n02087394-Rhodesian_ridgeback\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02088094-Afghan_hound.pickle.\n",
      "train/n02088094-Afghan_hound\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02088238-basset.pickle.\n",
      "train/n02088238-basset\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02088364-beagle.pickle.\n",
      "train/n02088364-beagle\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02088466-bloodhound.pickle.\n",
      "train/n02088466-bloodhound\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02088632-bluetick.pickle.\n",
      "train/n02088632-bluetick\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02089078-black-and-tan_coonhound.pickle.\n",
      "train/n02089078-black-and-tan_coonhound\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02089867-Walker_hound.pickle.\n",
      "train/n02089867-Walker_hound\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02089973-English_foxhound.pickle.\n",
      "train/n02089973-English_foxhound\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02090379-redbone.pickle.\n",
      "train/n02090379-redbone\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02090622-borzoi.pickle.\n",
      "train/n02090622-borzoi\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02090721-Irish_wolfhound.pickle.\n",
      "train/n02090721-Irish_wolfhound\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02091032-Italian_greyhound.pickle.\n",
      "train/n02091032-Italian_greyhound\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02091134-whippet.pickle.\n",
      "train/n02091134-whippet\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02091244-Ibizan_hound.pickle.\n",
      "train/n02091244-Ibizan_hound\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02091467-Norwegian_elkhound.pickle.\n",
      "train/n02091467-Norwegian_elkhound\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02091635-otterhound.pickle.\n",
      "train/n02091635-otterhound\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02091831-Saluki.pickle.\n",
      "train/n02091831-Saluki\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02092002-Scottish_deerhound.pickle.\n",
      "train/n02092002-Scottish_deerhound\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02092339-Weimaraner.pickle.\n",
      "train/n02092339-Weimaraner\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02093256-Staffordshire_bullterrier.pickle.\n",
      "train/n02093256-Staffordshire_bullterrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02093428-American_Staffordshire_terrier.pickle.\n",
      "train/n02093428-American_Staffordshire_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02093647-Bedlington_terrier.pickle.\n",
      "train/n02093647-Bedlington_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02093754-Border_terrier.pickle.\n",
      "train/n02093754-Border_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02093859-Kerry_blue_terrier.pickle.\n",
      "train/n02093859-Kerry_blue_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02093991-Irish_terrier.pickle.\n",
      "train/n02093991-Irish_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02094114-Norfolk_terrier.pickle.\n",
      "train/n02094114-Norfolk_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02094258-Norwich_terrier.pickle.\n",
      "train/n02094258-Norwich_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02094433-Yorkshire_terrier.pickle.\n",
      "train/n02094433-Yorkshire_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02095314-wire-haired_fox_terrier.pickle.\n",
      "train/n02095314-wire-haired_fox_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02095570-Lakeland_terrier.pickle.\n",
      "train/n02095570-Lakeland_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02095889-Sealyham_terrier.pickle.\n",
      "train/n02095889-Sealyham_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02096051-Airedale.pickle.\n",
      "train/n02096051-Airedale\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02096177-cairn.pickle.\n",
      "train/n02096177-cairn\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02096294-Australian_terrier.pickle.\n",
      "train/n02096294-Australian_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02096437-Dandie_Dinmont.pickle.\n",
      "train/n02096437-Dandie_Dinmont\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02096585-Boston_bull.pickle.\n",
      "train/n02096585-Boston_bull\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02097047-miniature_schnauzer.pickle.\n",
      "train/n02097047-miniature_schnauzer\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02097130-giant_schnauzer.pickle.\n",
      "train/n02097130-giant_schnauzer\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02097209-standard_schnauzer.pickle.\n",
      "train/n02097209-standard_schnauzer\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02097298-Scotch_terrier.pickle.\n",
      "train/n02097298-Scotch_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02097474-Tibetan_terrier.pickle.\n",
      "train/n02097474-Tibetan_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02097658-silky_terrier.pickle.\n",
      "train/n02097658-silky_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02098105-soft-coated_wheaten_terrier.pickle.\n",
      "train/n02098105-soft-coated_wheaten_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02098286-West_Highland_white_terrier.pickle.\n",
      "train/n02098286-West_Highland_white_terrier\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02098413-Lhasa.pickle.\n",
      "train/n02098413-Lhasa\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02099267-flat-coated_retriever.pickle.\n",
      "train/n02099267-flat-coated_retriever\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02099429-curly-coated_retriever.pickle.\n",
      "train/n02099429-curly-coated_retriever\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02099601-golden_retriever.pickle.\n",
      "train/n02099601-golden_retriever\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02099712-Labrador_retriever.pickle.\n",
      "train/n02099712-Labrador_retriever\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02099849-Chesapeake_Bay_retriever.pickle.\n",
      "train/n02099849-Chesapeake_Bay_retriever\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02100236-German_short-haired_pointer.pickle.\n",
      "train/n02100236-German_short-haired_pointer\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02100583-vizsla.pickle.\n",
      "train/n02100583-vizsla\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02100735-English_setter.pickle.\n",
      "train/n02100735-English_setter\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02100877-Irish_setter.pickle.\n",
      "train/n02100877-Irish_setter\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02101006-Gordon_setter.pickle.\n",
      "train/n02101006-Gordon_setter\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02101388-Brittany_spaniel.pickle.\n",
      "train/n02101388-Brittany_spaniel\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02101556-clumber.pickle.\n",
      "train/n02101556-clumber\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02102040-English_springer.pickle.\n",
      "train/n02102040-English_springer\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02102177-Welsh_springer_spaniel.pickle.\n",
      "train/n02102177-Welsh_springer_spaniel\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02102318-cocker_spaniel.pickle.\n",
      "train/n02102318-cocker_spaniel\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02102480-Sussex_spaniel.pickle.\n",
      "train/n02102480-Sussex_spaniel\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02102973-Irish_water_spaniel.pickle.\n",
      "train/n02102973-Irish_water_spaniel\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02104029-kuvasz.pickle.\n",
      "train/n02104029-kuvasz\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02104365-schipperke.pickle.\n",
      "train/n02104365-schipperke\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02105056-groenendael.pickle.\n",
      "train/n02105056-groenendael\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02105162-malinois.pickle.\n",
      "train/n02105162-malinois\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02105251-briard.pickle.\n",
      "train/n02105251-briard\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02105412-kelpie.pickle.\n",
      "train/n02105412-kelpie\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02105505-komondor.pickle.\n",
      "train/n02105505-komondor\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02105641-Old_English_sheepdog.pickle.\n",
      "train/n02105641-Old_English_sheepdog\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02105855-Shetland_sheepdog.pickle.\n",
      "train/n02105855-Shetland_sheepdog\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02106030-collie.pickle.\n",
      "train/n02106030-collie\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02106166-Border_collie.pickle.\n",
      "train/n02106166-Border_collie\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02106382-Bouvier_des_Flandres.pickle.\n",
      "train/n02106382-Bouvier_des_Flandres\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02106550-Rottweiler.pickle.\n",
      "train/n02106550-Rottweiler\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02106662-German_shepherd.pickle.\n",
      "train/n02106662-German_shepherd\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02107142-Doberman.pickle.\n",
      "train/n02107142-Doberman\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02107312-miniature_pinscher.pickle.\n",
      "train/n02107312-miniature_pinscher\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02107574-Greater_Swiss_Mountain_dog.pickle.\n",
      "train/n02107574-Greater_Swiss_Mountain_dog\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02107683-Bernese_mountain_dog.pickle.\n",
      "train/n02107683-Bernese_mountain_dog\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02107908-Appenzeller.pickle.\n",
      "train/n02107908-Appenzeller\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02108000-EntleBucher.pickle.\n",
      "train/n02108000-EntleBucher\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02108089-boxer.pickle.\n",
      "train/n02108089-boxer\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02108422-bull_mastiff.pickle.\n",
      "train/n02108422-bull_mastiff\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02108551-Tibetan_mastiff.pickle.\n",
      "train/n02108551-Tibetan_mastiff\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02108915-French_bulldog.pickle.\n",
      "train/n02108915-French_bulldog\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02109047-Great_Dane.pickle.\n",
      "train/n02109047-Great_Dane\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02109525-Saint_Bernard.pickle.\n",
      "train/n02109525-Saint_Bernard\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02109961-Eskimo_dog.pickle.\n",
      "train/n02109961-Eskimo_dog\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02110063-malamute.pickle.\n",
      "train/n02110063-malamute\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02110185-Siberian_husky.pickle.\n",
      "train/n02110185-Siberian_husky\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02110627-affenpinscher.pickle.\n",
      "train/n02110627-affenpinscher\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02110806-basenji.pickle.\n",
      "train/n02110806-basenji\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02110958-pug.pickle.\n",
      "train/n02110958-pug\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02111129-Leonberg.pickle.\n",
      "train/n02111129-Leonberg\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02111277-Newfoundland.pickle.\n",
      "train/n02111277-Newfoundland\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02111500-Great_Pyrenees.pickle.\n",
      "train/n02111500-Great_Pyrenees\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02111889-Samoyed.pickle.\n",
      "train/n02111889-Samoyed\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02112018-Pomeranian.pickle.\n",
      "train/n02112018-Pomeranian\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02112137-chow.pickle.\n",
      "train/n02112137-chow\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02112350-keeshond.pickle.\n",
      "train/n02112350-keeshond\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02112706-Brabancon_griffon.pickle.\n",
      "train/n02112706-Brabancon_griffon\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02113023-Pembroke.pickle.\n",
      "train/n02113023-Pembroke\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02113186-Cardigan.pickle.\n",
      "train/n02113186-Cardigan\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02113624-toy_poodle.pickle.\n",
      "train/n02113624-toy_poodle\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02113712-miniature_poodle.pickle.\n",
      "train/n02113712-miniature_poodle\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02113799-standard_poodle.pickle.\n",
      "train/n02113799-standard_poodle\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02113978-Mexican_hairless.pickle.\n",
      "train/n02113978-Mexican_hairless\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02115641-dingo.pickle.\n",
      "train/n02115641-dingo\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02115913-dhole.pickle.\n",
      "train/n02115913-dhole\n",
      "Full dataset tensor: (100, 224, 224, 3)\n",
      "Pickling train/n02116738-African_hunting_dog.pickle.\n",
      "train/n02116738-African_hunting_dog\n",
      "Full dataset tensor: (100, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "train_datasets = maybe_pickle(train_folders, force=True)\n",
    "#test_datasets = maybe_pickle(test_folders, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows,img_size, img_size,num_channels), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_datasets(pickle_files, train_size, valid_size=0, even_size=True):\n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "    \n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class+tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                breed_set = pickle.load(f)\n",
    "                np.random.shuffle(breed_set)\n",
    "                \n",
    "            if not even_size:\n",
    "                tsize_per_class,end_l = len(breed_set),len(breed_set)\n",
    "                end_t = start_t + tsize_per_class\n",
    "                \n",
    "            if valid_dataset is not None:\n",
    "                valid_breed = breed_set[:vsize_per_class, :, :, :]\n",
    "                valid_dataset[start_v:end_v, :, :, :] = valid_breed\n",
    "                valid_labels[start_v:end_v] = label\n",
    "                start_v += vsize_per_class\n",
    "                end_v += vsize_per_class\n",
    "\n",
    "            \n",
    "            train_breed = breed_set[vsize_per_class:end_l, :, :, :]\n",
    "            train_dataset[start_t:end_t, :, :, :] = train_breed\n",
    "            train_labels[start_t:end_t] = label\n",
    "            start_t += tsize_per_class\n",
    "            end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "    \n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (9600, 224, 224, 3) (9600,)\n",
      "Validation: (2400, 224, 224, 3) (2400,)\n"
     ]
    }
   ],
   "source": [
    "train_size = 9600\n",
    "valid_size = 2400\n",
    "test_size = 8580\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: (8580, 224, 224, 3) (8580,)\n"
     ]
    }
   ],
   "source": [
    "#_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size, even_size=False)\n",
    "#print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to run once error is cleared\n",
    "np.save(open('train_dataset.npy','wb'), train_dataset)\n",
    "np.save(open('train_labels.npy','wb'), train_labels)\n",
    "np.save(open('valid_dataset.npy','wb'), valid_dataset)\n",
    "np.save(open('valid_labels.npy','wb'), valid_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" already saved, don't run again\n",
    "\n",
    "np.save(open('test_dataset.npy','wb'), test_dataset)\n",
    "np.save(open('test_labels.npy','wb'), test_labels) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
